{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-BuXkcgf3Dj"
      },
      "source": [
        "## Using the typhoon-ocr package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrmJ00D5gS24"
      },
      "outputs": [],
      "source": [
        "!pip install typhoon-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfdVyX9YgVni"
      },
      "outputs": [],
      "source": [
        "# Skipping Linux-specific poppler-utils install; not needed for Windows and local image use.\n",
        "# !apt-get update && apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl2x4yNcgCf4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TYPHOON_OCR_API_KEY'] = \"sk-4IXy2viwPZnoAYVuYELYsFRkqNERSz5XFUNkmgQ3G1B4Miw6\" #You can get an API key from https://opentyphoon.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mISx3FAJgPtE"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, Markdown\n",
        "image_path = r'C:/Users/iHC/Desktop/OCRaft-main/BOMs.png'\n",
        "Image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTMeK90AgfOR"
      },
      "outputs": [],
      "source": [
        "from typhoon_ocr import ocr_document\n",
        "\n",
        "#You can experiment with two task_type options 'default' or 'structure' to see which one best suits your use case.\n",
        "\n",
        "markdown = ocr_document(image_path, task_type = \"default\")\n",
        "print(markdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ha08v4cgfxw"
      },
      "outputs": [],
      "source": [
        "markdown1 = ocr_document(image_path, task_type = \"structure\")\n",
        "print(markdown1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNf-FpbWxXlf"
      },
      "source": [
        "## Manually using either the API or a local model. (Full python code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ePQF7lwyVo2"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy\n",
        "!pip install pypdf\n",
        "!pip install pillow\n",
        "!pip install openai\n",
        "!pip install accelerate\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzzHat5tIwhA",
        "outputId": "1c8bf0fe-0abc-440d-f0d3-ef711937ff2d"
      },
      "outputs": [],
      "source": [
        "# Skipping Linux-specific poppler-utils install; not needed for Windows and local image use.\n",
        "# !apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2EskYc1wILm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is copied from https://github.com/allenai/olmocr\n",
        "Under the Apache 2.0 license.\n",
        "All credit goes to the original authors.\n",
        "\"\"\"\n",
        "from dataclasses import dataclass\n",
        "import re\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "import subprocess\n",
        "import base64\n",
        "from typing import List, Literal\n",
        "import random\n",
        "import ftfy\n",
        "from pypdf.generic import RectangleObject\n",
        "from pypdf import PdfReader\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Element:\n",
        "    pass\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BoundingBox:\n",
        "    x0: float\n",
        "    y0: float\n",
        "    x1: float\n",
        "    y1: float\n",
        "\n",
        "    @staticmethod\n",
        "    def from_rectangle(rect: RectangleObject) -> \"BoundingBox\":\n",
        "        return BoundingBox(rect[0], rect[1], rect[2], rect[3])\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TextElement(Element):\n",
        "    text: str\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ImageElement(Element):\n",
        "    name: str\n",
        "    bbox: BoundingBox\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PageReport:\n",
        "    mediabox: BoundingBox\n",
        "    text_elements: List[TextElement]\n",
        "    image_elements: List[ImageElement]\n",
        "\n",
        "def image_to_pdf(image_path):\n",
        "    try:\n",
        "        # Open the image file.\n",
        "        img = Image.open(image_path)\n",
        "        # Create a temporary file to store the PDF.\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "            filename = tmp.name\n",
        "            temp_pdf_created = True\n",
        "        # Convert image to RGB if necessary and save as PDF.\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        img.save(filename, \"PDF\")\n",
        "        return filename\n",
        "    except Exception as conv_err:\n",
        "        return None\n",
        "\n",
        "def get_pdf_media_box_width_height(local_pdf_path: str, page_num: int) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Get the MediaBox dimensions for a specific page in a PDF file using the pdfinfo command.\n",
        "\n",
        "    :param pdf_file: Path to the PDF file\n",
        "    :param page_num: The page number for which to extract MediaBox dimensions\n",
        "    :return: A dictionary containing MediaBox dimensions or None if not found\n",
        "    \"\"\"\n",
        "    # Construct the pdfinfo command to extract info for the specific page\n",
        "    command = [\"pdfinfo\", \"-f\", str(page_num), \"-l\", str(page_num), \"-box\", \"-enc\", \"UTF-8\", local_pdf_path]\n",
        "\n",
        "    # Run the command using subprocess\n",
        "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "    # Check if there is any error in executing the command\n",
        "    if result.returncode != 0:\n",
        "        raise ValueError(f\"Error running pdfinfo: {result.stderr}\")\n",
        "\n",
        "    # Parse the output to find MediaBox\n",
        "    output = result.stdout\n",
        "\n",
        "    for line in output.splitlines():\n",
        "        if \"MediaBox\" in line:\n",
        "            media_box_str: List[str] = line.split(\":\")[1].strip().split()\n",
        "            media_box: List[float] = [float(x) for x in media_box_str]\n",
        "            return abs(media_box[0] - media_box[2]), abs(media_box[3] - media_box[1])\n",
        "\n",
        "    raise ValueError(\"MediaBox not found in the PDF info.\")\n",
        "\n",
        "def render_pdf_to_base64png(local_pdf_path: str, page_num: int, target_longest_image_dim: int = 2048) -> str:\n",
        "    longest_dim = max(get_pdf_media_box_width_height(local_pdf_path, page_num))\n",
        "\n",
        "    # Convert PDF page to PNG using pdftoppm\n",
        "    pdftoppm_result = subprocess.run(\n",
        "        [\n",
        "            \"pdftoppm\",\n",
        "            \"-png\",\n",
        "            \"-f\",\n",
        "            str(page_num),\n",
        "            \"-l\",\n",
        "            str(page_num),\n",
        "            \"-r\",\n",
        "            str(target_longest_image_dim * 72 / longest_dim),  # 72 pixels per point is the conversion factor\n",
        "            local_pdf_path,\n",
        "        ],\n",
        "        timeout=120,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "    )\n",
        "    assert pdftoppm_result.returncode == 0, pdftoppm_result.stderr\n",
        "    return base64.b64encode(pdftoppm_result.stdout).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def _linearize_pdf_report(report: PageReport, max_length: int = 4000) -> str:\n",
        "    result = \"\"\n",
        "    result += f\"Page dimensions: {report.mediabox.x1:.1f}x{report.mediabox.y1:.1f}\\n\"\n",
        "\n",
        "    if max_length < 20:\n",
        "        return result\n",
        "\n",
        "    images = _merge_image_elements(report.image_elements)\n",
        "\n",
        "    # Process image elements\n",
        "    image_strings = []\n",
        "    for element in images:\n",
        "        image_str = f\"[Image {element.bbox.x0:.0f}x{element.bbox.y0:.0f} to {element.bbox.x1:.0f}x{element.bbox.y1:.0f}]\\n\"\n",
        "        # Use element's unique identifier (e.g., id or position) for comparison\n",
        "        image_strings.append((element, image_str))\n",
        "\n",
        "    # Process text elements\n",
        "    text_strings = []\n",
        "    for element in report.text_elements:  # type: ignore\n",
        "        if len(element.text.strip()) == 0:  # type: ignore\n",
        "            continue\n",
        "\n",
        "        element_text = _cleanup_element_text(element.text)  # type: ignore\n",
        "        text_str = f\"[{element.x:.0f}x{element.y:.0f}]{element_text}\\n\"  # type: ignore\n",
        "        text_strings.append((element, text_str))\n",
        "\n",
        "    # Combine all elements with their positions for sorting\n",
        "    all_elements: list[tuple[str, ImageElement, str, tuple[float, float]]] = []\n",
        "    for elem, s in image_strings:\n",
        "        position = (elem.bbox.x0, elem.bbox.y0)\n",
        "        all_elements.append((\"image\", elem, s, position))\n",
        "    for elem, s in text_strings:\n",
        "        position = (elem.x, elem.y)  # type: ignore\n",
        "        all_elements.append((\"text\", elem, s, position))\n",
        "\n",
        "    # Calculate total length\n",
        "    total_length = len(result) + sum(len(s) for _, _, s, _ in all_elements)\n",
        "\n",
        "    if total_length <= max_length:\n",
        "        # Include all elements\n",
        "        for _, _, s, _ in all_elements:\n",
        "            result += s\n",
        "        return result\n",
        "\n",
        "    # Identify elements with min/max coordinates\n",
        "    edge_elements = set()\n",
        "\n",
        "    if images:\n",
        "        min_x0_image = min(images, key=lambda e: e.bbox.x0)\n",
        "        max_x1_image = max(images, key=lambda e: e.bbox.x1)\n",
        "        min_y0_image = min(images, key=lambda e: e.bbox.y0)\n",
        "        max_y1_image = max(images, key=lambda e: e.bbox.y1)\n",
        "        edge_elements.update([min_x0_image, max_x1_image, min_y0_image, max_y1_image])\n",
        "\n",
        "    if report.text_elements:\n",
        "        text_elements = [e for e in report.text_elements if len(e.text.strip()) > 0]\n",
        "        if text_elements:\n",
        "            min_x_text = min(text_elements, key=lambda e: e.x)\n",
        "            max_x_text = max(text_elements, key=lambda e: e.x)\n",
        "            min_y_text = min(text_elements, key=lambda e: e.y)\n",
        "            max_y_text = max(text_elements, key=lambda e: e.y)\n",
        "            edge_elements.update([min_x_text, max_x_text, min_y_text, max_y_text])  # type: ignore\n",
        "\n",
        "    # Keep track of element IDs to prevent duplication\n",
        "    selected_element_ids = set()\n",
        "    selected_elements = []\n",
        "\n",
        "    # Include edge elements first\n",
        "    for elem_type, elem, s, position in all_elements:\n",
        "        if elem in edge_elements and id(elem) not in selected_element_ids:\n",
        "            selected_elements.append((elem_type, elem, s, position))\n",
        "            selected_element_ids.add(id(elem))\n",
        "\n",
        "    # Calculate remaining length\n",
        "    current_length = len(result) + sum(len(s) for _, _, s, _ in selected_elements)\n",
        "    _remaining_length = max_length - current_length\n",
        "\n",
        "    # Exclude edge elements from the pool\n",
        "    remaining_elements = [(elem_type, elem, s, position) for elem_type, elem, s, position in all_elements if id(elem) not in selected_element_ids]\n",
        "\n",
        "    # Sort remaining elements by their positions (e.g., x-coordinate and then y-coordinate)\n",
        "    # remaining_elements.sort(key=lambda x: (x[3][0], x[3][1]))\n",
        "\n",
        "    # Shuffle remaining elements randomly\n",
        "    random.shuffle(remaining_elements)\n",
        "\n",
        "    # Add elements until reaching max_length\n",
        "    for elem_type, elem, s, position in remaining_elements:\n",
        "        if current_length + len(s) > max_length:\n",
        "            break\n",
        "        selected_elements.append((elem_type, elem, s, position))\n",
        "        selected_element_ids.add(id(elem))\n",
        "        current_length += len(s)\n",
        "\n",
        "    # Sort selected elements by their positions to maintain logical order\n",
        "    selected_elements.sort(key=lambda x: (x[3][0], x[3][1]))\n",
        "\n",
        "    # Build the final result\n",
        "    for _, _, s, _ in selected_elements:\n",
        "        result += s\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _cap_split_string(text: str, max_length: int) -> str:\n",
        "    if len(text) <= max_length:\n",
        "        return text\n",
        "\n",
        "    head_length = max_length // 2 - 3\n",
        "    tail_length = head_length\n",
        "\n",
        "    head = text[:head_length].rsplit(\" \", 1)[0] or text[:head_length]\n",
        "    tail = text[-tail_length:].split(\" \", 1)[-1] or text[-tail_length:]\n",
        "\n",
        "    return f\"{head} ... {tail}\"\n",
        "\n",
        "\n",
        "def _cleanup_element_text(element_text: str) -> str:\n",
        "    MAX_TEXT_ELEMENT_LENGTH = 250\n",
        "    TEXT_REPLACEMENTS = {\"[\": \"\\\\[\", \"]\": \"\\\\]\", \"\\n\": \"\\\\n\", \"\\r\": \"\\\\r\", \"\\t\": \"\\\\t\"}\n",
        "    text_replacement_pattern = re.compile(\"|\".join(re.escape(key) for key in TEXT_REPLACEMENTS.keys()))\n",
        "\n",
        "    element_text = ftfy.fix_text(element_text).strip()\n",
        "\n",
        "    # Replace square brackets with escaped brackets and other escaped chars\n",
        "    element_text = text_replacement_pattern.sub(lambda match: TEXT_REPLACEMENTS[match.group(0)], element_text)\n",
        "\n",
        "    return _cap_split_string(element_text, MAX_TEXT_ELEMENT_LENGTH)\n",
        "\n",
        "def _merge_image_elements(images: List[ImageElement], tolerance: float = 0.5) -> List[ImageElement]:\n",
        "    n = len(images)\n",
        "    parent = list(range(n))  # Initialize Union-Find parent pointers\n",
        "\n",
        "    def find(i):\n",
        "        # Find with path compression\n",
        "        root = i\n",
        "        while parent[root] != root:\n",
        "            root = parent[root]\n",
        "        while parent[i] != i:\n",
        "            parent_i = parent[i]\n",
        "            parent[i] = root\n",
        "            i = parent_i\n",
        "        return root\n",
        "\n",
        "    def union(i, j):\n",
        "        # Union by attaching root of one tree to another\n",
        "        root_i = find(i)\n",
        "        root_j = find(j)\n",
        "        if root_i != root_j:\n",
        "            parent[root_i] = root_j\n",
        "\n",
        "    def bboxes_overlap(b1: BoundingBox, b2: BoundingBox, tolerance: float) -> bool:\n",
        "        # Compute horizontal and vertical distances between boxes\n",
        "        h_dist = max(0, max(b1.x0, b2.x0) - min(b1.x1, b2.x1))\n",
        "        v_dist = max(0, max(b1.y0, b2.y0) - min(b1.y1, b2.y1))\n",
        "        # Check if distances are within tolerance\n",
        "        return h_dist <= tolerance and v_dist <= tolerance\n",
        "\n",
        "    # Union overlapping images\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            if bboxes_overlap(images[i].bbox, images[j].bbox, tolerance):\n",
        "                union(i, j)\n",
        "\n",
        "    # Group images by their root parent\n",
        "    groups: dict[int, list[int]] = {}\n",
        "    for i in range(n):\n",
        "        root = find(i)\n",
        "        groups.setdefault(root, []).append(i)\n",
        "\n",
        "    # Merge images in the same group\n",
        "    merged_images = []\n",
        "    for indices in groups.values():\n",
        "        # Initialize merged bounding box\n",
        "        merged_bbox = images[indices[0]].bbox\n",
        "        merged_name = images[indices[0]].name\n",
        "\n",
        "        for idx in indices[1:]:\n",
        "            bbox = images[idx].bbox\n",
        "            # Expand merged_bbox to include the current bbox\n",
        "            merged_bbox = BoundingBox(\n",
        "                x0=min(merged_bbox.x0, bbox.x0),\n",
        "                y0=min(merged_bbox.y0, bbox.y0),\n",
        "                x1=max(merged_bbox.x1, bbox.x1),\n",
        "                y1=max(merged_bbox.y1, bbox.y1),\n",
        "            )\n",
        "            # Optionally, update the name\n",
        "            merged_name += f\"+{images[idx].name}\"\n",
        "\n",
        "        merged_images.append(ImageElement(name=merged_name, bbox=merged_bbox))\n",
        "\n",
        "    # Return the merged images along with other elements\n",
        "    return merged_images\n",
        "\n",
        "def _transform_point(x, y, m):\n",
        "    x_new = m[0] * x + m[2] * y + m[4]\n",
        "    y_new = m[1] * x + m[3] * y + m[5]\n",
        "    return x_new, y_new\n",
        "\n",
        "def _mult(m: List[float], n: List[float]) -> List[float]:\n",
        "    return [\n",
        "        m[0] * n[0] + m[1] * n[2],\n",
        "        m[0] * n[1] + m[1] * n[3],\n",
        "        m[2] * n[0] + m[3] * n[2],\n",
        "        m[2] * n[1] + m[3] * n[3],\n",
        "        m[4] * n[0] + m[5] * n[2] + n[4],\n",
        "        m[4] * n[1] + m[5] * n[3] + n[5],\n",
        "    ]\n",
        "\n",
        "def _pdf_report(local_pdf_path: str, page_num: int) -> PageReport:\n",
        "    reader = PdfReader(local_pdf_path)\n",
        "    page = reader.pages[page_num - 1]\n",
        "    resources = page.get(\"/Resources\", {})\n",
        "    xobjects = resources.get(\"/XObject\", {})\n",
        "    text_elements, image_elements = [], []\n",
        "\n",
        "    def visitor_body(text, cm, tm, font_dict, font_size):\n",
        "        txt2user = _mult(tm, cm)\n",
        "        text_elements.append(TextElement(text, txt2user[4], txt2user[5]))\n",
        "\n",
        "    def visitor_op(op, args, cm, tm):\n",
        "        if op == b\"Do\":\n",
        "            xobject_name = args[0]\n",
        "            xobject = xobjects.get(xobject_name)\n",
        "            if xobject and xobject[\"/Subtype\"] == \"/Image\":\n",
        "                # Compute image bbox\n",
        "                # The image is placed according to the CTM\n",
        "                _width = xobject.get(\"/Width\")\n",
        "                _height = xobject.get(\"/Height\")\n",
        "                x0, y0 = _transform_point(0, 0, cm)\n",
        "                x1, y1 = _transform_point(1, 1, cm)\n",
        "                image_elements.append(ImageElement(xobject_name, BoundingBox(min(x0, x1), min(y0, y1), max(x0, x1), max(y0, y1))))\n",
        "\n",
        "    page.extract_text(visitor_text=visitor_body, visitor_operand_before=visitor_op)\n",
        "\n",
        "    return PageReport(\n",
        "        mediabox=BoundingBox.from_rectangle(page.mediabox),\n",
        "        text_elements=text_elements,\n",
        "        image_elements=image_elements,\n",
        "    )\n",
        "\n",
        "def get_anchor_text(\n",
        "    local_pdf_path: str, page: int, pdf_engine: Literal[\"pdftotext\", \"pdfium\", \"pypdf\", \"topcoherency\", \"pdfreport\"], target_length: int = 4000\n",
        ") -> str:\n",
        "    assert page > 0, \"Pages are 1-indexed in pdf-land\"\n",
        "\n",
        "\n",
        "    if pdf_engine == \"pdfreport\":\n",
        "        return _linearize_pdf_report(_pdf_report(local_pdf_path, page), max_length=target_length)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unknown engine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz-WIux3y9Ai"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "PROMPTS_SYS = {\n",
        "    \"default\": lambda base_text: (f\"Below is an image of a document page along with its dimensions. \"\n",
        "        f\"Simply return the markdown representation of this document, presenting tables in markdown format as they naturally appear.\\n\"\n",
        "        f\"If the document contains images, use a placeholder like dummy.png for each image.\\n\"\n",
        "        f\"Your final output must be in JSON format with a single key `natural_text` containing the response.\\n\"\n",
        "        f\"RAW_TEXT_START\\n{base_text}\\nRAW_TEXT_END\"),\n",
        "    \"structure\": lambda base_text: (\n",
        "        f\"Below is an image of a document page, along with its dimensions and possibly some raw textual content previously extracted from it. \"\n",
        "        f\"Note that the text extraction may be incomplete or partially missing. Carefully consider both the layout and any available text to reconstruct the document accurately.\\n\"\n",
        "        f\"Your task is to return the markdown representation of this document, presenting tables in HTML format as they naturally appear.\\n\"\n",
        "        f\"If the document contains images or figures, analyze them and include the tag <figure>IMAGE_ANALYSIS</figure> in the appropriate location.\\n\"\n",
        "        f\"Your final output must be in JSON format with a single key `natural_text` containing the response.\\n\"\n",
        "        f\"RAW_TEXT_START\\n{base_text}\\nRAW_TEXT_END\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def get_prompt(prompt_name: str) -> Callable[[str], str]:\n",
        "    \"\"\"\n",
        "    Fetches the system prompt based on the provided PROMPT_NAME.\n",
        "\n",
        "    :param prompt_name: The identifier for the desired prompt.\n",
        "    :return: The system prompt as a string.\n",
        "    \"\"\"\n",
        "    return PROMPTS_SYS.get(prompt_name, lambda x: \"Invalid PROMPT_NAME provided.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNoD7m9lynvI"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoProcessor,Qwen2_5_VLForConditionalGeneration\n",
        "import torch\n",
        "from openai import OpenAI\n",
        "\n",
        "def process_pdf(pdf_or_image_file, page_num ,task_type, engine_call):\n",
        "    if pdf_or_image_file is None:\n",
        "        return None, \"No file uploaded\"\n",
        "\n",
        "    filename = pdf_or_image_file  # default to original file if PDF\n",
        "\n",
        "    # If the file is not a PDF, assume it's an image and convert it to PDF.\n",
        "    if not pdf_or_image_file.endswith(\".pdf\"):\n",
        "        filename = image_to_pdf(pdf_or_image_file)\n",
        "        print(filename)\n",
        "        if filename is None:\n",
        "            return None, \"Error converting image to PDF\"\n",
        "\n",
        "    # Render the first page to base64 PNG and then load it into a PIL image.\n",
        "    image_base64 = render_pdf_to_base64png(filename, page_num, target_longest_image_dim=1800)\n",
        "    image_pil = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
        "\n",
        "    # Extract anchor text from the PDF (first page)\n",
        "    anchor_text = get_anchor_text(filename, page_num, pdf_engine=\"pdfreport\", target_length=8000)\n",
        "\n",
        "    # Retrieve and fill in the prompt template with the anchor_text\n",
        "    prompt_template_fn = get_prompt(task_type)\n",
        "    PROMPT = prompt_template_fn(anchor_text)\n",
        "\n",
        "\n",
        "\n",
        "    # Create a messages structure including text and image URL\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": PROMPT},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
        "        ],\n",
        "    }]\n",
        "    # send messages to openai compatible api\n",
        "    if engine_call == \"api\":\n",
        "\n",
        "      openai = OpenAI(base_url=\"TYPHOON_BASE_URL\", api_key=\"TYPHOON_API_KEY\") #You can get an API key from https://opentyphoon.ai/\n",
        "\n",
        "      response = openai.chat.completions.create(\n",
        "          model=\"typhoon-ocr-preview\",\n",
        "          messages=messages,\n",
        "          max_tokens=16384,\n",
        "          extra_body={\n",
        "              \"repetition_penalty\": 1.2,\n",
        "              \"temperature\": 0.1,\n",
        "              \"top_p\": 0.6,\n",
        "          },\n",
        "\n",
        "      )\n",
        "      text_output = response.choices[0].message.content\n",
        "\n",
        "    else:\n",
        "\n",
        "      # Initialize the model\n",
        "      model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"scb10x/typhoon-ocr-7b\", torch_dtype=torch.bfloat16 ).eval()\n",
        "      processor = AutoProcessor.from_pretrained(\"scb10x/typhoon-ocr-7b\")\n",
        "\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      model.to(device)\n",
        "      # Apply the chat template and processor\n",
        "      text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "      main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
        "\n",
        "      inputs = processor(\n",
        "          text=[text],\n",
        "          images=[main_image],\n",
        "          padding=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
        "\n",
        "      # Generate the output\n",
        "      output = model.generate(\n",
        "                  **inputs,\n",
        "                  temperature=0.1,\n",
        "                  max_new_tokens=12000,\n",
        "                  num_return_sequences=1,\n",
        "                  repetition_penalty=1.2,\n",
        "                  do_sample=True,\n",
        "              )\n",
        "      # Decode the output\n",
        "      prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "      new_tokens = output[:, prompt_length:]\n",
        "      text_output = processor.tokenizer.batch_decode(\n",
        "          new_tokens, skip_special_tokens=True\n",
        "      )\n",
        "      text_output = text_output[0]\n",
        "\n",
        "    # Try to parse the output assuming it is a Python dictionary containing 'natural_text'\n",
        "    try:\n",
        "        json_data = json.loads(text_output)\n",
        "        markdown_out = json_data.get('natural_text', \"\").replace(\"<figure>\", \"\").replace(\"</figure>\", \"\")\n",
        "    except Exception as e:\n",
        "        markdown_out = f\"⚠️ Could not extract `natural_text` from output.\\nError: {str(e)}\"\n",
        "\n",
        "    return image_pil, markdown_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dXAFhcVKFFD"
      },
      "source": [
        "## In this demo, we use inference Typhon OCR via our API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ad5OFLmadQD"
      },
      "source": [
        "You can get an API key from https://opentyphoon.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-LH9Vv008WW"
      },
      "outputs": [],
      "source": [
        "pdf_or_image_file = \"/content/MDA_2016_Thai_v2.pdf\"\n",
        "page_num = 1 # In case of multi-page pdf, you can specific page number\n",
        "task_type = \"default\" # we support to type of document , you can explore on 'default' and 'structure'\n",
        "\n",
        "# Demo via API\n",
        "image_pil, markdown_out = process_pdf(pdf_or_image_file, page_num ,task_type, engine_call = \"api\")\n",
        "\n",
        "# Demo via local require GPU\n",
        "#image_pil, markdown_out = process_pdf(pdf_or_image_file, page_num ,task_type, engine_call = \"local\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hkjh4J99I_O5",
        "outputId": "bcb89172-852c-4607-8f84-350ad4e02463"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML, Markdown\n",
        "display(image_pil)\n",
        "display(Markdown(markdown_out))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
